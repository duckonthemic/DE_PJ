# ğŸ—„ï¸ Staging Layer Design

> **Document Version**: 1.0  
> **Last Updated**: December 2024  
> **Purpose**: Thiáº¿t káº¿ Staging Layer cho Data Lake

---

## 1. Tá»•ng Quan Staging Layer

### 1.1 Staging lÃ  gÃ¬?

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          STAGING LAYER CONCEPT                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚   Staging Layer (Bronze Layer) lÃ  táº§ng Ä‘áº§u tiÃªn trong Data Lake, cÃ³        â”‚
â”‚   nhiá»‡m vá»¥ lÆ°u trá»¯ dá»¯ liá»‡u RAW tá»« cÃ¡c nguá»“n mÃ  KHÃ”NG transform.            â”‚
â”‚                                                                             â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                          â”‚
â”‚   â”‚   SOURCE     â”‚                                                          â”‚
â”‚   â”‚  SYSTEMS     â”‚                                                          â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                                          â”‚
â”‚          â”‚                                                                  â”‚
â”‚          â”‚  Extract (khÃ´ng transform)                                       â”‚
â”‚          â”‚                                                                  â”‚
â”‚          â–¼                                                                  â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚   â”‚   STAGING    â”‚     â”‚  CHARACTERISTICS:                            â”‚    â”‚
â”‚   â”‚   (BRONZE)   â”‚     â”‚  â€¢ 1:1 copy tá»« source                        â”‚    â”‚
â”‚   â”‚              â”‚     â”‚  â€¢ Giá»¯ nguyÃªn schema gá»‘c                      â”‚    â”‚
â”‚   â”‚              â”‚     â”‚  â€¢ Partition theo snapshot_date               â”‚    â”‚
â”‚   â”‚              â”‚     â”‚  â€¢ CÃ³ thá»ƒ replay tá»« staging                   â”‚    â”‚
â”‚   â”‚              â”‚     â”‚  â€¢ Single Source of Truth cho raw data       â”‚    â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.2 Táº¡i sao cáº§n Staging Layer?

| LÃ½ do | Giáº£i thÃ­ch |
|-------|------------|
| **Decoupling** | TÃ¡ch biá»‡t source system khá»i processing. Source down khÃ´ng áº£nh hÆ°á»Ÿng analytics |
| **Replayability** | CÃ³ thá»ƒ re-process data náº¿u transform logic thay Ä‘á»•i |
| **Auditing** | Giá»¯ báº£n gá»‘c Ä‘á»ƒ audit, debug khi cÃ³ váº¥n Ä‘á» |
| **Performance** | Query staging khÃ´ng áº£nh hÆ°á»Ÿng source OLTP |
| **History** | Track thay Ä‘á»•i data theo thá»i gian |

---

## 2. Cáº¥u TrÃºc ThÆ° Má»¥c

### 2.1 Directory Structure

```
data/
â”œâ”€â”€ raw/                         # Dá»¯ liá»‡u thÃ´ chÆ°a xá»­ lÃ½ (optional)
â”‚   â””â”€â”€ .gitkeep
â”‚
â”œâ”€â”€ staging/                     # ğŸ”¥ BRONZE LAYER
â”‚   â”‚
â”‚   â”œâ”€â”€ snapshot_date=2024-01-01/
â”‚   â”‚   â”œâ”€â”€ categories.csv       # Full export
â”‚   â”‚   â”œâ”€â”€ products.csv
â”‚   â”‚   â”œâ”€â”€ customers.csv
â”‚   â”‚   â”œâ”€â”€ orders.csv
â”‚   â”‚   â”œâ”€â”€ order_items.csv
â”‚   â”‚   â”œâ”€â”€ payments.csv
â”‚   â”‚   â”œâ”€â”€ invoices.csv
â”‚   â”‚   â”œâ”€â”€ invoice_items.csv
â”‚   â”‚   â”œâ”€â”€ _metadata.json       # Pipeline metadata
â”‚   â”‚   â””â”€â”€ _SUCCESS             # Completion marker
â”‚   â”‚
â”‚   â”œâ”€â”€ snapshot_date=2024-01-02/
â”‚   â”‚   â””â”€â”€ ... (same structure)
â”‚   â”‚
â”‚   â””â”€â”€ snapshot_date=2024-01-03/
â”‚       â””â”€â”€ ...
â”‚
â”œâ”€â”€ processed/                   # SILVER LAYER (Sprint 2)
â”‚   â””â”€â”€ .gitkeep
â”‚
â””â”€â”€ gold/                        # GOLD LAYER - Marts (Sprint 3)
    â””â”€â”€ .gitkeep
```

### 2.2 Giáº£i thÃ­ch cÃ¡c thÃ nh pháº§n

| Component | Purpose | Example |
|-----------|---------|---------|
| `snapshot_date=YYYY-MM-DD` | Partition key (Hive-style) | `snapshot_date=2024-01-15` |
| `{table}.csv` | Data file | `customers.csv` |
| `_metadata.json` | Pipeline run info | Row counts, duration, errors |
| `_SUCCESS` | Completion marker | Empty file Ä‘Ã¡nh dáº¥u done |

---

## 3. Naming Convention

### 3.1 File Naming

```python
# Pattern: {table_name}.{format}

# Examples:
customers.csv
products.parquet
orders.csv
```

### 3.2 Partition Naming (Hive-style)

```python
# Pattern: {partition_key}={value}/

# Examples:
snapshot_date=2024-01-15/
snapshot_date=2024-01-16/

# Nhiá»u partition keys (future):
year=2024/month=01/day=15/
```

### 3.3 Táº¡i sao dÃ¹ng Hive-style?

```
Hive-style partitioning (key=value/) lÃ  standard trong Big Data:

âœ… Æ¯u Ä‘iá»ƒm:
  â€¢ Tá»± Ä‘á»™ng recognized bá»Ÿi Spark, Hive, Presto, Athena
  â€¢ Dá»… dÃ ng filter theo partition (partition pruning)
  â€¢ Human-readable
  â€¢ Self-documenting

âŒ Náº¿u KHÃ”NG dÃ¹ng Hive-style:
  data/staging/2024-01-15/customers.csv
  -> Tool khÃ´ng biáº¿t "2024-01-15" lÃ  gÃ¬
  -> Pháº£i custom code Ä‘á»ƒ parse
```

---

## 4. File Formats

### 4.1 CSV (Sprint 1)

```python
# Configuration khi export CSV
df.to_csv(
    file_path,
    index=False,              # KhÃ´ng lÆ°u index
    encoding='utf-8',         # Encoding chuáº©n
    date_format='%Y-%m-%d %H:%M:%S',  # ISO format
    na_rep='',                # NULL = empty string
    quoting=csv.QUOTE_MINIMAL  # Chá»‰ quote khi cáº§n
)
```

**Pros**:
- Human-readable
- Má»Ÿ Ä‘Æ°á»£c báº±ng Excel
- Dá»… debug

**Cons**:
- Lá»›n (khÃ´ng nÃ©n)
- KhÃ´ng cÃ³ schema
- Slow to read/write

### 4.2 Parquet (Recommended for Production)

```python
# Configuration khi export Parquet
df.to_parquet(
    file_path,
    index=False,
    engine='pyarrow',
    compression='snappy',     # Fast compression
    # compression='gzip',     # Better ratio but slower
)
```

**Pros**:
- Columnar format (query nhanh)
- Built-in compression (70-90% smaller)
- Schema embedded
- Industry standard

**Cons**:
- KhÃ´ng readable báº±ng text editor
- Cáº§n tool Ä‘á»ƒ view

### 4.3 Format Selection Guide

| Use Case | Recommended | Reason |
|----------|-------------|--------|
| Development/Debug | CSV | Dá»… xem, dá»… fix |
| Small data (<100MB) | CSV | KhÃ´ng cáº§n optimize |
| Large data (>100MB) | Parquet | Performance |
| Production | Parquet | Standard |
| Ad-hoc analysis | Parquet | Fast queries |

---

## 5. Metadata File

### 5.1 _metadata.json Structure

```json
{
  "pipeline": "source_to_staging",
  "snapshot_date": "2024-01-15",
  "run_timestamp": "2024-01-15T08:00:00+07:00",
  "duration_seconds": 125.5,
  "output_format": "csv",
  "source": {
    "host": "localhost",
    "database": "ecommerce_source",
    "schema": "ecommerce"
  },
  "tables": [
    {
      "table": "customers",
      "status": "success",
      "rows": 10000,
      "file": "customers.csv",
      "duration_seconds": 2.5
    },
    {
      "table": "orders",
      "status": "success",
      "rows": 100000,
      "file": "orders.csv",
      "duration_seconds": 45.2
    }
  ]
}
```

### 5.2 Purpose of Metadata

| Field | Use Case |
|-------|----------|
| `run_timestamp` | Audit: khi nÃ o pipeline cháº¡y |
| `duration_seconds` | Monitoring: pipeline cÃ³ cháº­m khÃ´ng |
| `rows` | Validation: so sÃ¡nh vá»›i source |
| `status` | Alerting: phÃ¡t hiá»‡n failures |

---

## 6. Data Quality Expectations

### 6.1 Staging Layer Quality Rules

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    STAGING QUALITY RULES                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚   MUST HAVE (Blocking):                                                     â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                     â”‚
â”‚   âœ… Row count staging = Row count source                                   â”‚
â”‚   âœ… No file corruption (file readable)                                     â”‚
â”‚   âœ… All tables exported                                                    â”‚
â”‚   âœ… _SUCCESS marker present                                                â”‚
â”‚                                                                             â”‚
â”‚   SHOULD HAVE (Warning):                                                    â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                    â”‚
â”‚   âš ï¸ Export time < threshold (e.g., 30 min)                                â”‚
â”‚   âš ï¸ File size within expected range                                       â”‚
â”‚   âš ï¸ Schema unchanged from previous run                                    â”‚
â”‚                                                                             â”‚
â”‚   NICE TO HAVE (Info):                                                      â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                       â”‚
â”‚   â„¹ï¸ Column statistics (nulls, distinct values)                            â”‚
â”‚   â„¹ï¸ Sample data validation                                                â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 6.2 Validation Queries

```python
# 1. Row count validation
source_count = db.execute("SELECT COUNT(*) FROM table")
staging_count = len(pd.read_csv("staging/table.csv"))
assert source_count == staging_count, f"Mismatch: {source_count} vs {staging_count}"

# 2. Schema validation
source_columns = set(db.execute("SELECT * FROM table LIMIT 0").columns)
staging_columns = set(pd.read_csv("staging/table.csv", nrows=0).columns)
assert source_columns == staging_columns, "Schema changed!"

# 3. Null check on required columns
staging_df = pd.read_csv("staging/customers.csv")
assert staging_df['email'].notna().all(), "Found NULL emails"
```

---

## 7. Best Practices

### 7.1 DO's âœ…

```
âœ… Partition by date (snapshot_date)
   â†’ Dá»… dÃ ng query theo thá»i gian
   â†’ CÃ³ thá»ƒ delete old partitions

âœ… Use consistent naming
   â†’ Table name in lowercase
   â†’ Use underscores, not spaces
   â†’ Same name as source table

âœ… Include metadata
   â†’ Row counts
   â†’ Timestamps
   â†’ Schema version

âœ… Use success markers
   â†’ _SUCCESS file
   â†’ Downstream jobs wait for this

âœ… Keep raw data immutable
   â†’ Never modify staging files
   â†’ Create new files instead
```

### 7.2 DON'Ts âŒ

```
âŒ DON'T transform data in staging
   â†’ Staging = exact copy of source
   â†’ Transform happens in SILVER layer

âŒ DON'T delete old snapshots without policy
   â†’ Keep at least 30 days
   â†’ Or based on storage policy

âŒ DON'T use spaces in file/folder names
   â†’ Bad: "Order Items.csv"
   â†’ Good: "order_items.csv"

âŒ DON'T mix formats in same layer
   â†’ All CSV or all Parquet
   â†’ Not mixed

âŒ DON'T hardcode paths
   â†’ Use environment variables
   â†’ Or config files
```

---

## 8. Usage Examples

### 8.1 Reading Staging Data

```python
import pandas as pd
from pathlib import Path

# Read latest snapshot
staging_path = Path("data/staging")
latest_snapshot = sorted(staging_path.glob("snapshot_date=*"))[-1]

# Read customers
customers = pd.read_csv(latest_snapshot / "customers.csv")

# Read orders
orders = pd.read_csv(latest_snapshot / "orders.csv", parse_dates=['order_date'])

print(f"Loaded {len(customers)} customers and {len(orders)} orders")
```

### 8.2 Finding Specific Snapshot

```python
# Read specific date
target_date = "2024-01-15"
snapshot_path = staging_path / f"snapshot_date={target_date}"

if snapshot_path.exists():
    df = pd.read_csv(snapshot_path / "orders.csv")
else:
    print(f"No snapshot for {target_date}")
```

### 8.3 Processing All Snapshots

```python
# Process all available snapshots
for snapshot in sorted(staging_path.glob("snapshot_date=*")):
    snapshot_date = snapshot.name.split("=")[1]
    
    # Check if success
    if not (snapshot / "_SUCCESS").exists():
        print(f"Skip {snapshot_date} - incomplete")
        continue
    
    # Process
    orders = pd.read_csv(snapshot / "orders.csv")
    print(f"{snapshot_date}: {len(orders)} orders")
```

---

## 9. Future Enhancements (Sprint 2+)

### 9.1 Incremental Loading

```python
# Instead of full load every day:
# SELECT * FROM orders

# Use incremental:
# SELECT * FROM orders WHERE updated_at > :last_run

# Staging structure with incremental:
# staging/snapshot_date=2024-01-15/orders_full.parquet     # Initial
# staging/snapshot_date=2024-01-16/orders_delta.parquet   # Only changes
```

### 9.2 Schema Evolution

```python
# Track schema changes
# staging/snapshot_date=2024-01-15/
#   â”œâ”€â”€ orders.parquet
#   â””â”€â”€ _schema/
#       â””â”€â”€ orders_schema.json  # Column names, types
```

### 9.3 Data Compaction

```python
# Compact small files into larger ones
# Before: 100 files x 1MB = 100MB
# After:  1 file x 100MB = 100MB (faster to read)
```

---

## 10. Checklist for Sprint 1

- [ ] Staging folder structure created
- [ ] Naming convention documented and followed
- [ ] Export script working (CSV format)
- [ ] Metadata file generated
- [ ] Success marker created
- [ ] Row count validation passing
- [ ] Documentation complete

---

> ğŸ“ **Note**: Staging layer design nÃ y phÃ¹ há»£p cho MVP vÃ  learning. Production system cÃ³ thá»ƒ cáº§n thÃªm features nhÆ° versioning, encryption, access control.
