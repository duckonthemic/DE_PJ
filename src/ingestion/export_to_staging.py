"""
===============================================================================
FILE: export_to_staging.py
PURPOSE: Export d·ªØ li·ªáu t·ª´ DB ngu·ªìn v√†o Staging Layer (Data Lake)
AUTHOR: Data Engineering Team
VERSION: 1.0

H∆Ø·ªöNG D·∫™N S·ª¨ D·ª§NG:
    # Full export t·∫•t c·∫£ tables
    python src/ingestion/export_to_staging.py
    
    # Export m·ªôt table c·ª• th·ªÉ
    python src/ingestion/export_to_staging.py --table orders
    
    # Export v·ªõi format parquet
    python src/ingestion/export_to_staging.py --format parquet
    
    # Export v·ªõi snapshot date c·ª• th·ªÉ
    python src/ingestion/export_to_staging.py --date 2024-01-15

KI·∫æN TR√öC:
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ  PostgreSQL     ‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫  ‚îÇ  Staging Layer  ‚îÇ
    ‚îÇ  (ecommerce)    ‚îÇ          ‚îÇ  (data/staging) ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ                              ‚îÇ
           ‚îÇ                              ‚ñº
           ‚îÇ                     snapshot_date=YYYY-MM-DD/
           ‚îÇ                          ‚îú‚îÄ‚îÄ customers.csv
           ‚îÇ                          ‚îú‚îÄ‚îÄ products.csv
           ‚îÇ                          ‚îú‚îÄ‚îÄ orders.csv
           ‚îÇ                          ‚îî‚îÄ‚îÄ ...
           ‚îÇ
           ‚ñº
    SELECT * FROM table
===============================================================================
"""

import os
import sys
import argparse
from datetime import datetime, date
from pathlib import Path
from typing import List, Dict, Optional
import logging
import json
import time

# Third-party imports
import pandas as pd
from sqlalchemy import create_engine, text
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# ============================================================================
# LOGGING CONFIGURATION
# ============================================================================

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)


# ============================================================================
# CONFIGURATION
# ============================================================================

class IngestConfig:
    """
    üí° GI·∫¢I TH√çCH:
    Configuration class cho pipeline ingest.
    T·∫≠p trung c√°c config ƒë·ªÉ d·ªÖ qu·∫£n l√Ω v√† thay ƒë·ªïi.
    """
    
    # Danh s√°ch tables c·∫ßn export (theo th·ª© t·ª± dependency)
    TABLES = [
        'categories',
        'products', 
        'customers',
        'orders',
        'order_items',
        'payments',
        'invoices',
        'invoice_items',
    ]
    
    # Schema c·ªßa source database
    SOURCE_SCHEMA = 'ecommerce'
    
    # Output formats h·ªó tr·ª£
    SUPPORTED_FORMATS = ['csv', 'parquet']
    
    # Default staging path
    STAGING_PATH = os.getenv('STAGING_PATH', './data/staging')


# ============================================================================
# DATABASE CONNECTION
# ============================================================================

class SourceDatabase:
    """
    üí° GI·∫¢I TH√çCH:
    Class qu·∫£n l√Ω k·∫øt n·ªëi ƒë·∫øn database ngu·ªìn.
    S·ª≠ d·ª•ng SQLAlchemy ƒë·ªÉ c√≥ th·ªÉ d·ªÖ d√†ng switch sang DB kh√°c.
    """
    
    def __init__(self):
        """Kh·ªüi t·∫°o connection t·ª´ environment variables"""
        self.host = os.getenv('SOURCE_DB_HOST', 'localhost')
        self.port = os.getenv('SOURCE_DB_PORT', '5432')
        self.database = os.getenv('SOURCE_DB_NAME', 'ecommerce_source')
        self.user = os.getenv('SOURCE_DB_USER', 'postgres')
        self.password = os.getenv('SOURCE_DB_PASSWORD', 'postgres')
        
        self.connection_string = (
            f"postgresql://{self.user}:{self.password}"
            f"@{self.host}:{self.port}/{self.database}"
        )
        self.engine = None
    
    def connect(self):
        """T·∫°o k·∫øt n·ªëi ƒë·∫øn database"""
        try:
            self.engine = create_engine(
                self.connection_string,
                # Connection pool settings
                pool_size=5,
                max_overflow=10,
                pool_pre_ping=True  # Ki·ªÉm tra connection c√≤n s·ªëng kh√¥ng
            )
            
            # Test connection
            with self.engine.connect() as conn:
                result = conn.execute(text("SELECT version()"))
                version = result.fetchone()[0]
                logger.info(f"‚úÖ Connected to: {self.database}")
                logger.debug(f"PostgreSQL version: {version}")
                
        except Exception as e:
            logger.error(f"‚ùå Failed to connect: {e}")
            raise
    
    def close(self):
        """ƒê√≥ng k·∫øt n·ªëi"""
        if self.engine:
            self.engine.dispose()
            logger.info("Database connection closed")
    
    def get_table_data(self, table_name: str, schema: str = 'ecommerce') -> pd.DataFrame:
        """
        ƒê·ªçc to√†n b·ªô d·ªØ li·ªáu t·ª´ m·ªôt table.
        
        üí° GI·∫¢I TH√çCH:
        Full load - ƒë·ªçc h·∫øt SELECT * FROM table
        Sprint 2 s·∫Ω implement incremental load v·ªõi WHERE updated_at > last_run
        
        Args:
            table_name: T√™n b·∫£ng
            schema: Schema name
            
        Returns:
            DataFrame ch·ª©a data
        """
        query = f"SELECT * FROM {schema}.{table_name}"
        
        try:
            df = pd.read_sql(query, self.engine)
            logger.info(f"Read {len(df)} rows from {schema}.{table_name}")
            return df
        except Exception as e:
            logger.error(f"Failed to read {table_name}: {e}")
            raise
    
    def get_row_count(self, table_name: str, schema: str = 'ecommerce') -> int:
        """L·∫•y s·ªë l∆∞·ª£ng rows trong table"""
        query = f"SELECT COUNT(*) FROM {schema}.{table_name}"
        with self.engine.connect() as conn:
            result = conn.execute(text(query))
            return result.fetchone()[0]


# ============================================================================
# STAGING LAYER
# ============================================================================

class StagingLayer:
    """
    üí° GI·∫¢I TH√çCH:
    Class qu·∫£n l√Ω Staging Layer - n∆°i l∆∞u tr·ªØ d·ªØ li·ªáu raw t·ª´ source.
    
    Staging Layer c√≥ nhi·ªám v·ª•:
    1. L∆∞u tr·ªØ d·ªØ li·ªáu raw (kh√¥ng transform)
    2. Partition theo snapshot_date ƒë·ªÉ track l·ªãch s·ª≠
    3. Cho ph√©p replay/recover khi c·∫ßn
    
    Structure:
        data/staging/
            snapshot_date=2024-01-01/
                customers.csv
                products.csv
                ...
            snapshot_date=2024-01-02/
                ...
    """
    
    def __init__(self, base_path: str, snapshot_date: date = None):
        """
        Args:
            base_path: ƒê∆∞·ªùng d·∫´n g·ªëc c·ªßa staging (e.g., ./data/staging)
            snapshot_date: Ng√†y snapshot, m·∫∑c ƒë·ªãnh l√† h√¥m nay
        """
        self.base_path = Path(base_path)
        self.snapshot_date = snapshot_date or date.today()
        
        # T·∫°o ƒë∆∞·ªùng d·∫´n cho snapshot n√†y
        self.snapshot_path = self.base_path / f"snapshot_date={self.snapshot_date.isoformat()}"
    
    def setup(self):
        """T·∫°o th∆∞ m·ª•c n·∫øu ch∆∞a t·ªìn t·∫°i"""
        self.snapshot_path.mkdir(parents=True, exist_ok=True)
        logger.info(f"Staging path: {self.snapshot_path}")
    
    def write_csv(self, df: pd.DataFrame, table_name: str) -> Path:
        """
        Ghi DataFrame ra file CSV.
        
        üí° GI·∫¢I TH√çCH:
        CSV ƒë∆∞·ª£c d√πng trong Sprint 1 v√¨:
        - D·ªÖ debug (m·ªü b·∫±ng Excel)
        - Human-readable
        - Kh√¥ng c·∫ßn c√†i th√™m th∆∞ vi·ªán
        
        Args:
            df: DataFrame c·∫ßn ghi
            table_name: T√™n table (l√†m t√™n file)
            
        Returns:
            Path ƒë·∫øn file ƒë√£ ghi
        """
        file_path = self.snapshot_path / f"{table_name}.csv"
        
        try:
            df.to_csv(
                file_path,
                index=False,
                encoding='utf-8',
                date_format='%Y-%m-%d %H:%M:%S'  # Format datetime chu·∫©n
            )
            logger.info(f"‚úÖ Written: {file_path} ({len(df)} rows)")
            return file_path
        except Exception as e:
            logger.error(f"‚ùå Failed to write {table_name}.csv: {e}")
            raise
    
    def write_parquet(self, df: pd.DataFrame, table_name: str) -> Path:
        """
        Ghi DataFrame ra file Parquet.
        
        üí° GI·∫¢I TH√çCH:
        Parquet ƒë∆∞·ª£c khuy√™n d√πng trong production v√¨:
        - N√©n t·ªët (70-90% smaller than CSV)
        - Columnar format (query nhanh)
        - Schema ƒë∆∞·ª£c l∆∞u trong file
        - H·ªó tr·ª£ partition t·ªët
        
        Args:
            df: DataFrame c·∫ßn ghi
            table_name: T√™n table
            
        Returns:
            Path ƒë·∫øn file ƒë√£ ghi
        """
        file_path = self.snapshot_path / f"{table_name}.parquet"
        
        try:
            df.to_parquet(
                file_path,
                index=False,
                engine='pyarrow',  # D√πng pyarrow engine
                compression='snappy'  # N√©n b·∫±ng snappy (nhanh, ratio t·ªët)
            )
            
            # Log file size comparison
            csv_size = len(df.to_csv(index=False).encode('utf-8'))
            parquet_size = file_path.stat().st_size
            compression_ratio = (1 - parquet_size / csv_size) * 100
            
            logger.info(f"‚úÖ Written: {file_path} ({len(df)} rows, {compression_ratio:.1f}% smaller than CSV)")
            return file_path
        except Exception as e:
            logger.error(f"‚ùå Failed to write {table_name}.parquet: {e}")
            raise
    
    def write_metadata(self, metadata: Dict) -> Path:
        """
        Ghi metadata file cho snapshot.
        
        üí° GI·∫¢I TH√çCH:
        Metadata gi√∫p track:
        - Th·ªùi ƒëi·ªÉm ch·∫°y pipeline
        - S·ªë rows m·ªói table
        - Version, errors...
        
        Args:
            metadata: Dict ch·ª©a th√¥ng tin
            
        Returns:
            Path ƒë·∫øn metadata file
        """
        file_path = self.snapshot_path / "_metadata.json"
        
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, indent=2, default=str)
        
        logger.info(f"Written metadata: {file_path}")
        return file_path
    
    def write_success_marker(self) -> Path:
        """
        T·∫°o _SUCCESS file ƒë√°nh d·∫•u export th√†nh c√¥ng.
        
        üí° GI·∫¢I TH√çCH:
        Pattern ph·ªï bi·∫øn trong data engineering:
        - Spark, Hadoop ƒë·ªÅu d√πng _SUCCESS marker
        - Downstream jobs ki·ªÉm tra file n√†y tr∆∞·ªõc khi ƒë·ªçc
        - N·∫øu kh√¥ng c√≥ = export ch∆∞a ho√†n th√†nh
        """
        file_path = self.snapshot_path / "_SUCCESS"
        file_path.touch()
        logger.info(f"Written success marker: {file_path}")
        return file_path


# ============================================================================
# INGEST PIPELINE
# ============================================================================

class IngestPipeline:
    """
    üí° GI·∫¢I TH√çCH:
    Main pipeline class ƒëi·ªÅu ph·ªëi to√†n b·ªô qu√° tr√¨nh ingest.
    
    Pipeline flow:
    1. Connect to source database
    2. For each table:
       a. Read data from source
       b. Write to staging layer
       c. Log results
    3. Write metadata
    4. Write success marker
    """
    
    def __init__(
        self,
        tables: List[str] = None,
        output_format: str = 'csv',
        snapshot_date: date = None,
        staging_path: str = None
    ):
        """
        Args:
            tables: List tables c·∫ßn export (None = all)
            output_format: 'csv' ho·∫∑c 'parquet'
            snapshot_date: Ng√†y snapshot
            staging_path: ƒê∆∞·ªùng d·∫´n staging
        """
        self.tables = tables or IngestConfig.TABLES
        self.output_format = output_format
        self.snapshot_date = snapshot_date or date.today()
        self.staging_path = staging_path or IngestConfig.STAGING_PATH
        
        # Validate format
        if output_format not in IngestConfig.SUPPORTED_FORMATS:
            raise ValueError(f"Format must be one of: {IngestConfig.SUPPORTED_FORMATS}")
        
        # Initialize components
        self.db = SourceDatabase()
        self.staging = StagingLayer(self.staging_path, self.snapshot_date)
        
        # Track results
        self.results = []
    
    def run(self) -> Dict:
        """
        Ch·∫°y pipeline ingest.
        
        Returns:
            Dict ch·ª©a k·∫øt qu·∫£ v√† th·ªëng k√™
        """
        logger.info("="*60)
        logger.info("Starting Ingest Pipeline")
        logger.info(f"Snapshot date: {self.snapshot_date}")
        logger.info(f"Output format: {self.output_format}")
        logger.info(f"Tables: {', '.join(self.tables)}")
        logger.info("="*60)
        
        start_time = time.time()
        
        try:
            # Setup
            self.db.connect()
            self.staging.setup()
            
            # Export each table
            for table in self.tables:
                self._export_table(table)
            
            # Write metadata
            duration = time.time() - start_time
            metadata = self._create_metadata(duration)
            self.staging.write_metadata(metadata)
            
            # Write success marker
            self.staging.write_success_marker()
            
            # Summary
            self._print_summary(duration)
            
            return {
                'success': True,
                'snapshot_date': self.snapshot_date.isoformat(),
                'duration_seconds': round(duration, 2),
                'tables': self.results
            }
            
        except Exception as e:
            logger.error(f"Pipeline failed: {e}")
            return {
                'success': False,
                'error': str(e),
                'tables': self.results
            }
        finally:
            self.db.close()
    
    def _export_table(self, table_name: str):
        """
        Export m·ªôt table t·ª´ source sang staging.
        
        Args:
            table_name: T√™n table c·∫ßn export
        """
        logger.info(f"\nüì¶ Exporting: {table_name}")
        start_time = time.time()
        
        try:
            # Read from source
            df = self.db.get_table_data(table_name)
            
            # Write to staging
            if self.output_format == 'csv':
                output_path = self.staging.write_csv(df, table_name)
            else:
                output_path = self.staging.write_parquet(df, table_name)
            
            duration = time.time() - start_time
            
            # Record result
            self.results.append({
                'table': table_name,
                'status': 'success',
                'rows': len(df),
                'file': str(output_path),
                'duration_seconds': round(duration, 2)
            })
            
        except Exception as e:
            logger.error(f"Failed to export {table_name}: {e}")
            self.results.append({
                'table': table_name,
                'status': 'failed',
                'error': str(e)
            })
            raise
    
    def _create_metadata(self, duration: float) -> Dict:
        """T·∫°o metadata dict"""
        return {
            'pipeline': 'source_to_staging',
            'snapshot_date': self.snapshot_date.isoformat(),
            'run_timestamp': datetime.now().isoformat(),
            'duration_seconds': round(duration, 2),
            'output_format': self.output_format,
            'source': {
                'host': self.db.host,
                'database': self.db.database,
                'schema': IngestConfig.SOURCE_SCHEMA
            },
            'tables': self.results
        }
    
    def _print_summary(self, duration: float):
        """In summary sau khi ch·∫°y xong"""
        logger.info("\n" + "="*60)
        logger.info("‚úÖ Ingest Pipeline Completed")
        logger.info("="*60)
        
        total_rows = sum(r.get('rows', 0) for r in self.results)
        success_count = sum(1 for r in self.results if r['status'] == 'success')
        
        logger.info(f"Duration: {duration:.2f} seconds")
        logger.info(f"Tables: {success_count}/{len(self.results)} successful")
        logger.info(f"Total rows: {total_rows:,}")
        logger.info(f"Output: {self.staging.snapshot_path}")
        
        # Table-by-table summary
        logger.info("\nüìä Table Summary:")
        for result in self.results:
            status_icon = "‚úÖ" if result['status'] == 'success' else "‚ùå"
            rows = result.get('rows', 0)
            logger.info(f"  {status_icon} {result['table']}: {rows:,} rows")


# ============================================================================
# DATA VALIDATION
# ============================================================================

class DataValidator:
    """
    üí° GI·∫¢I TH√çCH:
    Class ƒë·ªÉ validate d·ªØ li·ªáu sau khi export.
    ƒê√¢y l√† ph·∫ßn quan tr·ªçng c·ªßa QC/QA trong data engineering.
    
    C√°c lo·∫°i validation:
    1. Row count: S·ªë rows trong staging = source
    2. Schema: C√°c c·ªôt ƒë·∫ßy ƒë·ªß v√† ƒë√∫ng type
    3. Null check: C√°c c·ªôt required kh√¥ng c√≥ null
    4. Sample check: Spot check v√†i rows
    """
    
    def __init__(self, db: SourceDatabase, staging: StagingLayer):
        self.db = db
        self.staging = staging
    
    def validate_row_counts(self) -> List[Dict]:
        """
        So s√°nh row count gi·ªØa source v√† staging.
        
        Returns:
            List c√°c validation results
        """
        results = []
        
        for table in IngestConfig.TABLES:
            # Source count
            source_count = self.db.get_row_count(table)
            
            # Staging count
            staging_file = self.staging.snapshot_path / f"{table}.csv"
            if staging_file.exists():
                staging_df = pd.read_csv(staging_file)
                staging_count = len(staging_df)
            else:
                staging_file = self.staging.snapshot_path / f"{table}.parquet"
                if staging_file.exists():
                    staging_df = pd.read_parquet(staging_file)
                    staging_count = len(staging_df)
                else:
                    staging_count = None
            
            # Compare
            match = source_count == staging_count if staging_count is not None else False
            
            results.append({
                'table': table,
                'source_count': source_count,
                'staging_count': staging_count,
                'match': match,
                'status': 'PASS' if match else 'FAIL'
            })
            
            status_icon = "‚úÖ" if match else "‚ùå"
            logger.info(f"{status_icon} {table}: source={source_count}, staging={staging_count}")
        
        return results
    
    def validate_sample(self, table: str, sample_size: int = 5) -> pd.DataFrame:
        """
        L·∫•y sample t·ª´ source v√† staging ƒë·ªÉ so s√°nh manual.
        
        Args:
            table: T√™n table
            sample_size: S·ªë rows ƒë·ªÉ sample
            
        Returns:
            DataFrame comparison
        """
        # Source sample
        source_df = self.db.get_table_data(table).head(sample_size)
        
        # Staging sample
        staging_file = self.staging.snapshot_path / f"{table}.csv"
        if staging_file.exists():
            staging_df = pd.read_csv(staging_file).head(sample_size)
        else:
            staging_df = pd.read_parquet(
                self.staging.snapshot_path / f"{table}.parquet"
            ).head(sample_size)
        
        return source_df, staging_df


# ============================================================================
# CLI INTERFACE
# ============================================================================

def parse_args():
    """
    Parse command line arguments.
    
    üí° GI·∫¢I TH√çCH:
    argparse cho ph√©p script nh·∫≠n tham s·ªë t·ª´ command line.
    V√≠ d·ª•:
        python export_to_staging.py --table orders --format parquet
    """
    parser = argparse.ArgumentParser(
        description='Export data from source database to staging layer',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
    # Export all tables as CSV
    python export_to_staging.py
    
    # Export specific table
    python export_to_staging.py --table orders
    
    # Export as Parquet
    python export_to_staging.py --format parquet
    
    # Export with specific date
    python export_to_staging.py --date 2024-01-15
    
    # Validate after export
    python export_to_staging.py --validate
        """
    )
    
    parser.add_argument(
        '--table', '-t',
        type=str,
        help='Specific table to export (default: all tables)'
    )
    
    parser.add_argument(
        '--format', '-f',
        type=str,
        default='csv',
        choices=['csv', 'parquet'],
        help='Output format (default: csv)'
    )
    
    parser.add_argument(
        '--date', '-d',
        type=str,
        help='Snapshot date in YYYY-MM-DD format (default: today)'
    )
    
    parser.add_argument(
        '--validate', '-v',
        action='store_true',
        help='Run validation after export'
    )
    
    parser.add_argument(
        '--staging-path',
        type=str,
        default=IngestConfig.STAGING_PATH,
        help=f'Staging layer path (default: {IngestConfig.STAGING_PATH})'
    )
    
    return parser.parse_args()


def main():
    """Main entry point"""
    args = parse_args()
    
    # Parse snapshot date
    snapshot_date = None
    if args.date:
        try:
            snapshot_date = datetime.strptime(args.date, '%Y-%m-%d').date()
        except ValueError:
            logger.error(f"Invalid date format: {args.date}. Use YYYY-MM-DD")
            sys.exit(1)
    
    # Determine tables
    tables = [args.table] if args.table else None
    
    # Run pipeline
    pipeline = IngestPipeline(
        tables=tables,
        output_format=args.format,
        snapshot_date=snapshot_date,
        staging_path=args.staging_path
    )
    
    result = pipeline.run()
    
    # Run validation if requested
    if args.validate and result['success']:
        logger.info("\n" + "="*60)
        logger.info("Running Validation")
        logger.info("="*60)
        
        validator = DataValidator(pipeline.db, pipeline.staging)
        
        # Reconnect for validation
        pipeline.db.connect()
        validation_results = validator.validate_row_counts()
        pipeline.db.close()
        
        # Check if all passed
        all_passed = all(r['status'] == 'PASS' for r in validation_results)
        
        if all_passed:
            logger.info("\n‚úÖ All validations PASSED")
        else:
            logger.warning("\n‚ö†Ô∏è Some validations FAILED")
            sys.exit(1)
    
    # Exit with appropriate code
    sys.exit(0 if result['success'] else 1)


if __name__ == "__main__":
    main()
